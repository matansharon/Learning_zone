{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mrdbourke/pytorch-deep-learning/blob/main/extras/exercises/02_pytorch_classification_exercises.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKJFt7YxH8yl"
      },
      "source": [
        "# 02. PyTorch Classification Exercises\n",
        "\n",
        "The following is a template for 02. PyTorch Classification exercises.\n",
        "\n",
        "It's only starter code and it's your job to fill in the blanks.\n",
        "\n",
        "Because of the flexibility of PyTorch, there may be more than one way to answer the question.\n",
        "\n",
        "Don't worry about trying to be *right* just try writing code that suffices the question.\n",
        "\n",
        "## Resources\n",
        "* These exercises are based on [notebook 02 of the learn PyTorch course](https://www.learnpytorch.io/02_pytorch_classification/).\n",
        "* You can see one form of [solutions on GitHub](https://github.com/mrdbourke/pytorch-deep-learning/tree/main/extras/solutions) (but try the exercises below yourself first!)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CSrUPgapO0tf"
      },
      "outputs": [],
      "source": [
        "# Import torch\n",
        "import torch\n",
        "\n",
        "# Setup device agnostic code\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Setup random seed\n",
        "RANDOM_SEED = 42"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pH7jIZ2SPFee"
      },
      "source": [
        "## 1. Make a binary classification dataset with Scikit-Learn's [`make_moons()`](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_moons.html) function.\n",
        "  * For consistency, the dataset should have 1000 samples and a `random_state=42`.\n",
        "  * Turn the data into PyTorch tensors. \n",
        "  * Split the data into training and test sets using `train_test_split` with 80% training and 20% testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5t4VhPV1PX1X"
      },
      "outputs": [],
      "source": [
        "# Create a dataset with Scikit-Learn's make_moons()\n",
        "from sklearn.datasets import make_moons\n",
        "n_samples=1000\n",
        "X, y = make_moons(n_samples=n_samples, noise=0.1, random_state=RANDOM_SEED)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SUeHZ3-3P9C7"
      },
      "outputs": [],
      "source": [
        "# Turn data into a DataFrame\n",
        "import pandas as pd\n",
        "# Create a Pandas DataFrame with the data (X) and the labels (y)\n",
        "df = pd.DataFrame(dict(x0=X[:,0], x1=X[:,1], y=y))\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "owrkPSFvQPFI"
      },
      "outputs": [],
      "source": [
        "# Visualize the data on a scatter plot\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "plt.figure(figsize=(10,8))\n",
        "plt.scatter(X[:,0], X[:,1], c=y, cmap=plt.cm.RdBu)\n",
        "plt.title(\"Classification of Non-Linear Data\")\n",
        "plt.xlabel(\"x0\")\n",
        "plt.ylabel(\"x1\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bDhyHn9fR4dq"
      },
      "outputs": [],
      "source": [
        "# Turn data into tensors of dtype float\n",
        "X = torch.tensor(X, dtype=torch.long)\n",
        "y = torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "# Split the data into train and test sets (80% train, 20% test)\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
        "                                                    test_size=0.2,\n",
        "                                                    random_state=RANDOM_SEED)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMIjxZdzQfPz"
      },
      "source": [
        "## 2. Build a model by subclassing `nn.Module` that incorporates non-linear activation functions and is capable of fitting the data you created in 1.\n",
        "  * Feel free to use any combination of PyTorch layers (linear and non-linear) you want."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hwtyvm34Ri6Q"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "# Inherit from nn.Module to make a model capable of fitting the moon data\n",
        "class MoonModelV0(nn.Module):\n",
        "    def __init__(self,in_features,hidden_units,n_output):\n",
        "        super().__init__()\n",
        "        # Define 3 layers\n",
        "        self.layer1 = nn.Linear(in_features=in_features, \n",
        "                                 out_features=hidden_units)\n",
        "        self.layer2=nn.Linear(in_features=hidden_units,\n",
        "                              out_features=n_output)\n",
        "        self.layer3=nn.Linear(in_features=n_output,\n",
        "                              out_features=n_output)\n",
        "        # Define sigmoid activation and softmax output\n",
        "        self.relu = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        \n",
        "        \n",
        "    \n",
        "\n",
        "    def forward(self, x):\n",
        "        # Define forward pass\n",
        "        x = self.layer1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.relu(x)\n",
        "        x=self.layer3(x)\n",
        "        x=self.sigmoid(x)\n",
        "        x=self.softmax(x)\n",
        "\n",
        "        \n",
        "        \n",
        "        return x\n",
        "\n",
        "# Instantiate the model\n",
        "model = MoonModelV0(in_features=2, hidden_units=5, n_output=2)\n",
        "model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSj97RwyVeFE"
      },
      "source": [
        "## 3. Setup a binary classification compatible loss function and optimizer to use when training the model built in 2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "whSGw5qgVvxU"
      },
      "outputs": [],
      "source": [
        "# Setup loss function\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "# Setup optimizer to optimize model's parameters\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvk4PfNTWUAt"
      },
      "source": [
        "## 4. Create a training and testing loop to fit the model you created in 2 to the data you created in 1.\n",
        "  * Do a forward pass of the model to see what's coming out in the form of logits, prediction probabilities and labels.\n",
        "  * To measure model accuray, you can create your own accuracy function or use the accuracy function in [TorchMetrics](https://torchmetrics.readthedocs.io/en/latest/).\n",
        "  * Train the model for long enough for it to reach over 96% accuracy.\n",
        "  * The training loop should output progress every 10 epochs of the model's training and test set loss and accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AgnFdlamd2-D",
        "outputId": "627d8c33-071e-4925-f18b-5d5ba6126729"
      },
      "outputs": [],
      "source": [
        "# # What's coming out of our model?\n",
        "# print(model(X_train))\n",
        "\n",
        "# # How does it compare to the actual labels?\n",
        "# print(y_train)\n",
        "\n",
        "# # logits (raw outputs of model)\n",
        "# print(\"Logits:\")\n",
        "\n",
        "# ## Your code here ##\n",
        "\n",
        "# # Prediction probabilities\n",
        "# print(\"Pred probs:\")\n",
        "# ## Your code here ##\n",
        "\n",
        "# # Prediction labels\n",
        "# print(\"Pred labels:\")\n",
        "# ## Your code here ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rUSDNHB4euoJ"
      },
      "outputs": [],
      "source": [
        "# Let's calculuate the accuracy using accuracy from TorchMetrics\n",
        "# !pip -q install torchmetrics # Colab doesn't come with torchmetrics\n",
        "from torchmetrics import Accuracy\n",
        "\n",
        "## TODO: Uncomment this code to use the Accuracy function\n",
        "acc_fn = Accuracy(task=\"multiclass\", num_classes=2).to(device) # send accuracy function to device\n",
        "acc_fn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SHBY3h7XXnxt"
      },
      "outputs": [],
      "source": [
        "# ## TODO: Uncomment this to set the seed\n",
        "# torch.manual_seed(RANDOM_SEED)\n",
        "\n",
        "# # Setup epochs\n",
        "# epochs = 1000\n",
        "\n",
        "# # Send data to the device\n",
        "# X_train = X_train.to(device)\n",
        "# y_train = y_train.to(device)\n",
        "# X_test = X_test.to(device)\n",
        "# y_test = y_test.to(device)\n",
        "\n",
        "# # Loop through the data\n",
        "# for epoch in range(epochs):\n",
        "#   ### Training\n",
        "#   # Set model to training mode\n",
        "#   model.train()\n",
        "  \n",
        "#   # 1. Forward pass (logits output)\n",
        "#   y_logits = model(X_train).squeeze() # Turn data into predictions\n",
        "  \n",
        "#   # Turn logits into prediction probabilities\n",
        "#   y_pred = torch.softmax(y_logits, dim=1) # Turn prediction probabilities into prediction labels\n",
        "  \n",
        "#   # 2. Calculaute the loss\n",
        "#   loss = loss_function(y_logits, y_train.long()) # loss = compare model raw outputs to desired model outputs\n",
        "  \n",
        "#   # Turn prediction probabilities into prediction labels\n",
        "#   y_pred = torch.softmax(y_logits, dim=1) # Turn prediction probabilities into prediction labels\n",
        "  \n",
        "\n",
        "#   # 2. Calculaute the loss\n",
        "#   loss = loss_function(y_logits, y_train.long()) # loss = compare model raw outputs to desired model outputs\n",
        "\n",
        "#   # Calculate the accuracy\n",
        "#   acc = acc_fn(y_pred, y_train.int()) # the accuracy function needs to compare pred labels (not logits) with actual labels\n",
        "\n",
        "#   # 3. Zero the gradients\n",
        "#   optimizer.zero_grad()\n",
        "  \n",
        "\n",
        "#   # 4. Loss backward (perform backpropagation) - https://brilliant.org/wiki/backpropagation/#:~:text=Backpropagation%2C%20short%20for%20%22backward%20propagation,to%20the%20neural%20network's%20weights.\n",
        "#   loss.backward()\n",
        "  \n",
        "#   # 5. Step the optimizer (gradient descent) - https://towardsdatascience.com/gradient-descent-algorithm-a-deep-dive-cf04e8115f21#:~:text=Gradient%20descent%20(GD)%20is%20an,e.g.%20in%20a%20linear%20regression) \n",
        "#   optimizer.step()\n",
        "\n",
        "#   ## Testing\n",
        "#   # Convert the data type of y_train to float\n",
        "#   y_train = y_train.float()\n",
        "\n",
        "#   # Print out what's happening every 100 epochs\n",
        "#   # if epoch % 100 == 0:\n",
        "#   # Convert the data type of y_train to float\n",
        "#   y_train = y_train.float()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "mat1 and mat2 must have the same dtype, but got Long and Float",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[63], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Forward pass (logits output)\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m y_logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze()  \u001b[38;5;66;03m# Turn data into predictions\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Calculate the loss\u001b[39;00m\n\u001b[1;32m     26\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_function(y_logits, y_train\u001b[38;5;241m.\u001b[39mlong())  \u001b[38;5;66;03m# Loss = compare model raw outputs to desired model outputs\u001b[39;00m\n",
            "File \u001b[0;32m~/python/Data_science/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/python/Data_science/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[50], line 25\u001b[0m, in \u001b[0;36mMoonModelV0.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;66;03m# Define forward pass\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(x)\n\u001b[1;32m     27\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer2(x)\n",
            "File \u001b[0;32m~/python/Data_science/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/python/Data_science/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/python/Data_science/.venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 must have the same dtype, but got Long and Float"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Assuming RANDOM_SEED, model, loss_function, optimizer, acc_fn, and device are defined elsewhere\n",
        "RANDOM_SEED = 42\n",
        "torch.manual_seed(RANDOM_SEED)  # Uncommented to set the seed\n",
        "\n",
        "# Setup epochs\n",
        "epochs = 1000\n",
        "\n",
        "# Send data to the device\n",
        "X_train = X_train.to(device)\n",
        "y_train = y_train.to(device)\n",
        "X_test = X_test.to(device)\n",
        "y_test = y_test.to(device)\n",
        "\n",
        "# Loop through the data\n",
        "for epoch in range(epochs):\n",
        "    ### Training\n",
        "    # Set model to training mode\n",
        "    model.train()\n",
        "  \n",
        "    # Forward pass (logits output)\n",
        "    y_logits = model(X_train).squeeze()  # Turn data into predictions\n",
        "  \n",
        "    # Calculate the loss\n",
        "    loss = loss_function(y_logits, y_train.long())  # Loss = compare model raw outputs to desired model outputs\n",
        "  \n",
        "    # Calculate prediction probabilities and labels for accuracy calculation\n",
        "    y_pred_probs = torch.softmax(y_logits, dim=1)  # Prediction probabilities\n",
        "    _, y_pred_labels = torch.max(y_pred_probs, 1)  # Prediction labels\n",
        "  \n",
        "    # Calculate the accuracy\n",
        "    acc = acc_fn(y_pred_labels, y_train)  # Assuming acc_fn compares labels directly\n",
        "\n",
        "    # Zero the gradients\n",
        "    optimizer.zero_grad()\n",
        "  \n",
        "    # Perform backpropagation\n",
        "    loss.backward()\n",
        "  \n",
        "    # Step the optimizer (gradient descent)\n",
        "    optimizer.step()\n",
        "\n",
        "    # Print out what's happening every 100 epochs\n",
        "    if epoch % 100 == 0:\n",
        "        print(f\"Epoch {epoch}: Loss = {loss.item()}, Accuracy = {acc.item()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Nwihtomj9JO"
      },
      "source": [
        "## 5. Make predictions with your trained model and plot them using the `plot_decision_boundary()` function created in this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0YRzatb8a1P2"
      },
      "outputs": [],
      "source": [
        "# Plot the model predictions\n",
        "import numpy as np\n",
        "\n",
        "def plot_decision_boundary(model, X, y):\n",
        "  \n",
        "    # Put everything to CPU (works better with NumPy + Matplotlib)\n",
        "    model.to(\"cpu\")\n",
        "    X, y = X.to(\"cpu\"), y.to(\"cpu\")\n",
        "\n",
        "    # Source - https://madewithml.com/courses/foundations/neural-networks/ \n",
        "    # (with modifications)\n",
        "    x_min, x_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1\n",
        "    y_min, y_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1\n",
        "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 101), \n",
        "                         np.linspace(y_min, y_max, 101))\n",
        "\n",
        "    # Make features\n",
        "    X_to_pred_on = torch.from_numpy(np.column_stack((xx.ravel(), yy.ravel()))).float()\n",
        "\n",
        "    # Make predictions\n",
        "    model.eval()\n",
        "    with torch.inference_mode():\n",
        "        y_logits = model(X_to_pred_on)\n",
        "\n",
        "    # Test for multi-class or binary and adjust logits to prediction labels\n",
        "    if len(torch.unique(y)) > 2:\n",
        "        y_pred = torch.softmax(y_logits, dim=1).argmax(dim=1) # mutli-class\n",
        "    else: \n",
        "        y_pred = torch.round(torch.sigmoid(y_logits)) # binary\n",
        "    \n",
        "    # Reshape preds and plot\n",
        "    y_pred = y_pred.reshape(xx.shape).detach().numpy()\n",
        "    plt.contourf(xx, yy, y_pred, cmap=plt.cm.RdYlBu, alpha=0.7)\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.RdYlBu)\n",
        "    plt.xlim(xx.min(), xx.max())\n",
        "    plt.ylim(yy.min(), yy.max())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PMrcpyirig1d"
      },
      "outputs": [],
      "source": [
        "# Plot decision boundaries for training and test sets\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtMYBvtciiAU"
      },
      "source": [
        "## 6. Replicate the Tanh (hyperbolic tangent) activation function in pure PyTorch.\n",
        "  * Feel free to reference the [ML cheatsheet website](https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html#tanh) for the formula."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BlXaWC5TkEUE"
      },
      "outputs": [],
      "source": [
        "# Create a straight line tensor\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vZPCcQmIkZjO"
      },
      "outputs": [],
      "source": [
        "# Test torch.tanh() on the tensor and plot it\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J-ne__Kjkdc1"
      },
      "outputs": [],
      "source": [
        "# Replicate torch.tanh() and plot it\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lbt1bNcWk5G9"
      },
      "source": [
        "## 7. Create a multi-class dataset using the [spirals data creation function from CS231n](https://cs231n.github.io/neural-networks-case-study/) (see below for the code).\n",
        "  * Split the data into training and test sets (80% train, 20% test) as well as turn it into PyTorch tensors.\n",
        "  * Construct a model capable of fitting the data (you may need a combination of linear and non-linear layers).\n",
        "  * Build a loss function and optimizer capable of handling multi-class data (optional extension: use the Adam optimizer instead of SGD, you may have to experiment with different values of the learning rate to get it working).\n",
        "  * Make a training and testing loop for the multi-class data and train a model on it to reach over 95% testing accuracy (you can use any accuracy measuring function here that you like) - 1000 epochs should be plenty.\n",
        "  * Plot the decision boundaries on the spirals dataset from your model predictions, the `plot_decision_boundary()` function should work for this dataset too."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "tU-UNZsKlJls",
        "outputId": "8b7b745a-070d-4ecb-c639-c4ee4d8eae06"
      },
      "outputs": [],
      "source": [
        "# Code for creating a spiral dataset from CS231n\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "N = 100 # number of points per class\n",
        "D = 2 # dimensionality\n",
        "K = 3 # number of classes\n",
        "X = np.zeros((N*K,D)) # data matrix (each row = single example)\n",
        "y = np.zeros(N*K, dtype='uint8') # class labels\n",
        "for j in range(K):\n",
        "  ix = range(N*j,N*(j+1))\n",
        "  r = np.linspace(0.0,1,N) # radius\n",
        "  t = np.linspace(j*4,(j+1)*4,N) + np.random.randn(N)*0.2 # theta\n",
        "  X[ix] = np.c_[r*np.sin(t), r*np.cos(t)]\n",
        "  y[ix] = j\n",
        "# lets visualize the data\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.RdYlBu)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OWVrmkEyl0VP"
      },
      "outputs": [],
      "source": [
        "# Turn data into tensors\n",
        "import torch\n",
        "X = torch.from_numpy(X).type(torch.float) # features as float32\n",
        "y = torch.from_numpy(y).type(torch.LongTensor) # labels need to be of type long\n",
        "\n",
        "# Create train and test splits\n",
        "from sklearn.model_selection import train_test_split\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-v-7f0op0tG"
      },
      "outputs": [],
      "source": [
        "# Let's calculuate the accuracy for when we fit our model\n",
        "!pip -q install torchmetrics # colab doesn't come with torchmetrics\n",
        "from torchmetrics import Accuracy\n",
        "\n",
        "## TODO: uncomment the two lines below to send the accuracy function to the device\n",
        "# acc_fn = Accuracy(task=\"multiclass\", num_classes=4).to(device)\n",
        "# acc_fn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DB3u3ldumapf"
      },
      "outputs": [],
      "source": [
        "# Prepare device agnostic code\n",
        "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Create model by subclassing nn.Module\n",
        "\n",
        "\n",
        "\n",
        "# Instantiate model and send it to device\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QE7XWSSunMTS",
        "outputId": "00b31909-87c9-41e3-9dbb-fb4c4bd3aabd"
      },
      "outputs": [],
      "source": [
        "# Setup data to be device agnostic\n",
        "\n",
        "\n",
        "# Print out first 10 untrained model outputs (forward pass)\n",
        "print(\"Logits:\")\n",
        "## Your code here ##\n",
        "\n",
        "print(\"Pred probs:\")\n",
        "## Your code here ##\n",
        "\n",
        "print(\"Pred labels:\")\n",
        "## Your code here ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "54EqLRKLo0AW"
      },
      "outputs": [],
      "source": [
        "# Setup loss function and optimizer\n",
        "# loss_fn =\n",
        "# optimizer = "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vIlExkUHnmxi"
      },
      "outputs": [],
      "source": [
        "# Build a training loop for the model\n",
        "\n",
        "# Loop over data\n",
        "\n",
        "\n",
        "  ## Training\n",
        "  \n",
        "  # 1. Forward pass\n",
        "  \n",
        "\n",
        "  # 2. Calculate the loss\n",
        "  \n",
        "  \n",
        "  # 3. Optimizer zero grad\n",
        "  \n",
        "\n",
        "  # 4. Loss backward\n",
        "  \n",
        "\n",
        "  # 5. Optimizer step\n",
        "  \n",
        "\n",
        "  ## Testing\n",
        "  \n",
        "\n",
        "    # 1. Forward pass\n",
        "    \n",
        "    # 2. Caculate loss and acc\n",
        "    \n",
        "  # Print out what's happening every 100 epochs\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JrwVRbaE0keT"
      },
      "outputs": [],
      "source": [
        "# Plot decision boundaries for training and test sets\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyNloicnciRwCXd2bJo6F2iS",
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "02_pytorch_classification_exercises.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
